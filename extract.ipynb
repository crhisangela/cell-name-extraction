{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O conteúdo do arquivo artigos/art2.pdf foi salvo em texto_extraido.txt\n"
     ]
    }
   ],
   "source": [
    "def pdf_to_text(pdf_path, txt_path):\n",
    "    text = extract_text(pdf_path)\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "        txt_file.write(text) \n",
    "\n",
    "    print(f\"O conteúdo do arquivo {pdf_path} foi salvo em {txt_path}\")\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "pdf_path = \"artigos/art2.pdf\"\n",
    "txt_path = \"texto_extraido.txt\"\n",
    "\n",
    "pdf_to_text(pdf_path, txt_path)\n",
    "\n",
    "def read_txt(caminho_arquivo):\n",
    "    with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:\n",
    "        return arquivo.read()\n",
    "\n",
    "text = read_txt(txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article subscriber access provided by kansas state university libraries marked improvement in photoinduced cell death by a new tris-heteroleptic complex with dual action: singlet oxygen sensitization and ligand dissociation bryan a. albani, bruno peña, nicholas a leed, nataly a. b. g. de paula, christiane pavani, mauricio s. baptista, kim r. dunbar, and claudia turro j. am. chem. soc., just accepted manuscript • doi: 10.1021/ja508272h • publication date (web): 13 nov 2014 downloaded from http://\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palavra</th>\n",
       "      <th>Frequência</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complexes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lysed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Palavra  Frequência\n",
       "0          1           3\n",
       "1  complexes           1\n",
       "2          3           6\n",
       "3      lysed           1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def extract_xxx_count(text, ignore_list=None):\n",
    "    # Lista de padrões regex\n",
    "    patterns = [\n",
    "        r'of\\s+(\\w+)\\s+observed',\n",
    "        r'properties of\\s+(\\w+)',\n",
    "        r'form of\\s+(\\w+)',\n",
    "        r'fluorescence for the\\s+(\\w+)',\n",
    "        r'spectr\\w*\\s+of\\s+(\\w+)',\n",
    "        r'into\\s+(\\w+)',\n",
    "        r'substituents of\\s+(\\w+)\\s+might be',\n",
    "        r'(\\w+)\\s+is a sensitizer',\n",
    "        r'concentrations of\\s+(\\w+)\\s+and',\n",
    "        r'the\\s+(\\w+)\\s+solution',\n",
    "        r'properties of\\s+(\\w+)\\s+as a',\n",
    "        r'[Ee]ffect of\\s+(\\w+)\\s+on the',\n",
    "        r'the\\s+(\\w+)\\s+equilibria',\n",
    "        r'complex\\s+(\\w+)\\s+was synthesized',\n",
    "        r'concentrations of\\s+(\\w+)',\n",
    "        r'the total\\s+(\\w+)\\s+concentrations'\n",
    "    ]\n",
    "    \n",
    "    matches = []\n",
    "    for pattern in patterns:\n",
    "        matches.extend(re.findall(pattern, text))  # Adiciona todas as correspondências encontradas\n",
    "    \n",
    "    # Filtrar palavras a serem ignoradas\n",
    "    if ignore_list:\n",
    "        matches = [word for word in matches if word.lower() not in ignore_list]\n",
    "\n",
    "    # Contar a frequência de cada palavra encontrada\n",
    "    counter = Counter(matches)\n",
    "    \n",
    "    # Converter para DataFrame\n",
    "    df = pd.DataFrame(counter.items(), columns=['Palavra', 'Frequência'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "ignore_list = ['the', 'all', 'a', 'multiple', 'an', 'both'] \n",
    "\n",
    "result_df = extract_xxx_count(text, ignore_list)\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
